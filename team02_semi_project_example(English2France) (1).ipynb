{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"team02_semi_project_example(English2France).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPi/XQr9m0qDJMuKL1zhwoR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KwIBieS22TQ4","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9tWb96o89o4U","colab":{}},"source":["#구글 드라이버를 마운트\n","from google.colab import drive\n","\n","drive.mount('gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LJD8vHwI9o4c","colab":{}},"source":["!ls -al \"/content/gdrive/My Drive/multicampus_ai/semi_project/workspace/data/seq2seq/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJcI-9seyOlK","colab_type":"text"},"source":["#데이터 읽어오기\n","\n","* fra.txt 데이터는 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조가 1줄\n","* 16만개의 병렬 문장 샘플을 포함\n","* src는 source의 줄임말로 입력 문장(영어)\n","* tar는 target의 줄임말로 번역하고자 하는 문장(불어)"]},{"cell_type":"code","metadata":{"id":"v0lTj1NtGefr","colab_type":"code","colab":{}},"source":["import pandas as pd\n","lines= pd.read_csv('/content/gdrive/My Drive/multicampus_ai/semi_project/workspace/data/seq2seq/fra.txt', names=['src', 'tar',\"desc\"], sep='\\t')\n","len(lines)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBbd8yepGlNB","colab_type":"code","colab":{}},"source":["lines = lines[0:60000] # 6만개만 저장\n","lines.sample(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVMPvJRgy_55","colab_type":"code","colab":{}},"source":["#desc 컬럼 삭제\n","lines.pop(\"desc\")\n","lines.sample(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nJ-CN4Z8zx6G","colab_type":"text"},"source":["lines.tar :line의 tar 컬럼 의 각 줄의 앞에 tab(\"\\t) 마지막에 엔터 (\"\\n) 을 추가해서 문장의 처음과 끝을 표시"]},{"cell_type":"code","metadata":{"id":"EVipZlQDGpmg","colab_type":"code","colab":{}},"source":["lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n","lines.sample(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwBYbCQ35LvT","colab_type":"text"},"source":["#텍스트 데이터 전처리"]},{"cell_type":"code","metadata":{"id":"cnJ6s65iGtLX","colab_type":"code","colab":{}},"source":["# 글자 집합 구축\n","#set()은 리스트와 같이 복수의 데이터를 저장 할 수 있는객체\n","# 리스트와 다른 점은 이미 존재하는 데이터를 중복해서 추가시 나중에 추가되는 데이터는 추가하지 않고 무시\n","#예를 들어서 set객체에 1,2 가 저장되 있는데 1을 또다시 중복해서 추가하려고 하면 나중에 저장하려는 1은 추가되지 않고 무시\n","src_vocab=set()\n","#lines의 src컬럼을 1줄씩 읽어서 line에 대입\n","for line in lines.src:\n","    #line에 단어 1개씩 읽어서 char에 대입\n","    for char in line: \n","        #src_vocab 에 char 추가 (중복 제거)\n","        src_vocab.add(char)\n","\n","tar_vocab=set()\n","for line in lines.tar:\n","    for char in line:\n","        tar_vocab.add(char)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bn0Vrmge2OPJ","colab_type":"code","colab":{}},"source":["print(\"src_vocab:\",src_vocab)\n","print(\"tar_vocab:\",tar_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVpQp7YxGz0u","colab_type":"code","colab":{}},"source":["#입력글자의 수를 src_vocab_size에 대입\n","#입력 글자 수는 len(src_vocab) 전체 글자수 \n","# src_vocab에 서 학습하지 않은 글자는 모두 같은 글자로 취급해서 +1\n","src_vocab_size = len(src_vocab)+1\n","tar_vocab_size = len(tar_vocab)+1\n","print(src_vocab_size)\n","print(tar_vocab_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpSrBEUXG3jF","colab_type":"code","colab":{}},"source":["# src_vocab 을 정렬\n","src_vocab = sorted(list(src_vocab))\n","#tar_vocab 을 정렬\n","tar_vocab = sorted(list(tar_vocab))\n","print(\"src_vocab:\",src_vocab)\n","print(\"tar_vocab:\",tar_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1Q4lSWHG-LE","colab_type":"code","colab":{}},"source":["#for i, word in enumerate(src_vocab): src_vocab에 저장된 글자에 인덱스를 붙여서 리턴\n","#인덱스는 i 글자는 word에 저장\n","\n","dict([(word, i+1) ..]\n","#단어를 key로  인덱스를 value로 사전 생성\n","src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n","tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n","print(src_to_index)\n","print(tar_to_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"By6sVYBSG-xr","colab_type":"code","colab":{}},"source":["encoder_input = []\n","for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n","    temp_X = []\n","    for w in line: #각 줄에서 1개씩 글자를 읽음\n","      #src_to_index 사전을 이용해서  글자를 해당되는 정수로 변환\n","      temp_X.append(src_to_index[w]) \n","    encoder_input.append(temp_X)\n","    \n","print(encoder_input[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"udYa-f7OKX51","colab_type":"code","colab":{}},"source":["#tar 문장의 글자들을 숫자로 변환\n","decoder_input = []\n","for line in lines.tar:\n","    temp_X = []\n","    for w in line:\n","      temp_X.append(tar_to_index[w])\n","    decoder_input.append(temp_X)\n","print(decoder_input[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CA8NsgmZHBZj","colab_type":"code","colab":{}},"source":["#tar 문장의 두번째 글자 부터 숫자로 변환\n","decoder_target = []\n","for line in lines.tar:\n","    t=0\n","    temp_X = []\n","    for w in line:\n","      if t>0:\n","        temp_X.append(tar_to_index[w])\n","      t=t+1\n","    decoder_target.append(temp_X)\n","\n","print(decoder_target[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IS72fJUfHI86","colab_type":"code","colab":{}},"source":["#line in lines.src: lines.src 의 각 줄을 line에 저장 len(line) \n","#len(line)의 글자수의 최대 값을 max_src_len에 대입\n","max_src_len = max([len(line) for line in lines.src])\n","max_tar_len = max([len(line) for line in lines.tar])\n","print(max_src_len)\n","print(max_tar_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCnAP7BRHLSh","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","#incoder_input: 입력 글자가 숫자로 변환되서 저장된 배열 예)[30, 64, 10]\n","#max_src_len: incoder_input의 최대 글자수\n","#pad_sequences(encoder_input, maxlen=max_src_len, padding='post'): encoder_input 뒤에 (padding='post') 0을 붙여서 배열 크기를 max_src_len 으로 만들어줌\n","#예)[30, 64, 10] => [30,60,10,0,0,.....0]\n","encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n","decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n","decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"myBRUnqJHN1J","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","#to_categorical(encoder_input) :encoder_input을 one hot 인코딩함\n","encoder_input = to_categorical(encoder_input)\n","decoder_input = to_categorical(decoder_input)\n","decoder_target = to_categorical(decoder_target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grrp31Oo5VII","colab_type":"text"},"source":["#모델 생성"]},{"cell_type":"markdown","metadata":{"id":"6GMRnAv65XhY","colab_type":"text"},"source":["인코더 모델 생성"]},{"cell_type":"code","metadata":{"id":"EGetoWsFHPyB","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n","from tensorflow.keras.models import Model\n","\n","encoder_inputs = Input(shape=(None, src_vocab_size))\n","encoder_lstm = LSTM(units=256, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n","# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n","encoder_states = [state_h, state_c]\n","# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dVvWNeH5bGi","colab_type":"text"},"source":["디코더 모델 생성"]},{"cell_type":"code","metadata":{"id":"aRqAo4A3HS5p","colab_type":"code","colab":{}},"source":["decoder_inputs = Input(shape=(None, tar_vocab_size))\n","decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n","decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n","decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n","decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iAgrr1Dg5dhH","colab_type":"text"},"source":["모델 학습 (epochs=3)"]},{"cell_type":"code","metadata":{"id":"o-eg0XxHHVio","colab_type":"code","colab":{}},"source":["#model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)\n","model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=3, validation_split=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-rfHnpQ6pXY","colab_type":"text"},"source":["#모델 사용하기"]},{"cell_type":"code","metadata":{"id":"bHybwstgHX5f","colab_type":"code","colab":{}},"source":["encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrng-NAQHaMf","colab_type":"code","colab":{}},"source":["# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n","decoder_states = [state_h, state_c]\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n","decoder_outputs = decoder_softmax_layer(decoder_outputs)\n","decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuCHFNW3Hczv","colab_type":"code","colab":{}},"source":["index_to_src = dict((i, char) for char, i in src_to_index.items())\n","index_to_tar = dict((i, char) for char, i in tar_to_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iohYuEDHe_m","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","    # 입력으로부터 인코더의 상태를 얻음\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # <SOS>에 해당하는 원-핫 벡터 생성\n","    target_seq = np.zeros((1, 1, tar_vocab_size))\n","    target_seq[0, 0, tar_to_index['\\t']] = 1.\n","\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","\n","    # stop_condition이 True가 될 때까지 루프 반복\n","    while not stop_condition:\n","        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # 예측 결과를 문자로 변환\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = index_to_tar[sampled_token_index]\n","\n","        # 현재 시점의 예측 문자를 예측 문장에 추가\n","        decoded_sentence += sampled_char\n","\n","        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_tar_len):\n","            stop_condition = True\n","\n","        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","        target_seq = np.zeros((1, 1, tar_vocab_size))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYH8BqQPHh9H","colab_type":"code","colab":{}},"source":["import numpy as np\n","for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n","    input_seq = encoder_input[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print(35 * \"-\")\n","    print('입력 문장:', lines.src[seq_index])\n","    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n","    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnh6c5JwKjnD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}